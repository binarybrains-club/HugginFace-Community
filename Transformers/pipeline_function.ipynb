{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92f78b5d",
   "metadata": {
    "id": "92f78b5d"
   },
   "source": [
    "# Funci√≥n Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d443e568",
   "metadata": {
    "id": "d443e568"
   },
   "source": [
    "El objeto m√°s b√°sico en la Librer√≠a Transformers es la funci√≥n \"pipeline()\"ü§ó\n",
    "\n",
    "Este conecta a un modelo a trav√©s de un atajo que te permite usar modelos de Hugging Face ya entrenados sin tener que preocuparte por todo el c√≥digo de preprocesamiento y posprocesamiento.\n",
    "\n",
    "En otras palabras, envuelve un modelo junto con su tokenizador y los pasos necesarios para que solo le des un input y recibas una salida lista."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda22ba4",
   "metadata": {
    "id": "cda22ba4"
   },
   "source": [
    "### Instalaci√≥n de Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "YbKpfkreN6nL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YbKpfkreN6nL",
    "outputId": "537dee74-a210-4f58-ac91-bc27aba0e380"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.24.2)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.24.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: typer>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (0.24.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
      "Requirement already satisfied: click>=8.2.1 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers) (8.3.1)\n",
      "Requirement already satisfied: rich>=12.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers) (13.9.4)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers) (0.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7df8d9",
   "metadata": {
    "id": "2f7df8d9"
   },
   "source": [
    "### Uso de Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6fa70",
   "metadata": {
    "id": "2ee6fa70"
   },
   "source": [
    "üöÄ Tareas soportadas por `pipeline()` en Hugging Face Transformers ü§ó\n",
    "\n",
    "üìÑ Pipelines de Texto\n",
    "- **text-generation** ‚Üí Generar texto a partir de un prompt.  \n",
    "- **text-classification** ‚Üí Clasificar texto en categor√≠as predefinidas.  \n",
    "- **summarization** ‚Üí Crear una versi√≥n m√°s corta de un texto manteniendo la informaci√≥n clave.  \n",
    "- **translation** ‚Üí Traducir texto de un idioma a otro.  \n",
    "- **zero-shot-classification** ‚Üí Clasificar texto sin entrenamiento previo en etiquetas espec√≠ficas.  \n",
    "- **feature-extraction** ‚Üí Extraer representaciones vectoriales de un texto.  \n",
    "- **fill-mask** ‚Üí Predecir la palabra faltante en una oraci√≥n\n",
    "- **question-answering** ‚Üí Responder preguntas d√°ndole un \"context\" y la \"question\"\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "üñºÔ∏è Pipelines de Imagen\n",
    "- **image-to-text** ‚Üí Generar descripciones de texto a partir de im√°genes.  \n",
    "- **image-classification** ‚Üí Identificar objetos en una imagen.  \n",
    "- **object-detection** ‚Üí Localizar e identificar objetos en im√°genes.  \n",
    "\n",
    "---\n",
    "\n",
    "üîä Pipelines de Audio\n",
    "- **automatic-speech-recognition** ‚Üí Convertir voz en texto.  \n",
    "- **audio-classification** ‚Üí Clasificar audio en categor√≠as.  \n",
    "- **text-to-speech** ‚Üí Convertir texto en audio hablado.  \n",
    "\n",
    "---\n",
    "\n",
    "üéØ Pipelines Multimodales\n",
    "- **image-text-to-text** ‚Üí Responder a una imagen bas√°ndose en un prompt de texto.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa2f750f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "a2b64606b9c745479f3fb220ab0b8948",
      "3e739ef571c1472ca164f1c4a27005ab",
      "f6c53469945948c0802be20d6ee62487",
      "7799e0bca28e4fc8885c4ae87e38370d",
      "86650b5c1d1645429a9daad9687fe1d5",
      "0167b36799254a7ea007ab71b4ebddac",
      "b15c51f20ada4c9eb75a511093ce664a",
      "40a093d9f0dc452691c4746f63192900",
      "0298783b5d184e1b99d2ba2e2f43af67",
      "b1d205150fab451daf97452ae80392a3",
      "f2f7b5bb4e914f6e8eb36b55cca8f62e"
     ]
    },
    "id": "aa2f750f",
    "outputId": "25f34859-de4a-480c-b097-d1540263f828"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f.\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b64606b9c745479f3fb220ab0b8948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9989345669746399}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"I love using transformers library!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57ed9000",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191,
     "referenced_widgets": [
      "669f394398314b6dad5ee346b2df3404",
      "17d8322046824dae93f4d2391d121d4c",
      "8a9a3644706441a0a9aca5ef7b95ff17",
      "107a07ac2693472ebe843de164509076",
      "98e4184b049143e0b6af94f843e8fd62",
      "94ed3934e3404e9c9e74898de437d7c0",
      "7e1a6484a3a044b1b9cfdc612c77cb4d",
      "008f6a2b95d54ff88ba7e74191e15a8d",
      "8015514d014743738a7b2646b0e4d18e",
      "6b8723b8ca23460e89812d9cf30fe8da",
      "e0b0e0c4b04045e683de3f70c02c91bc"
     ]
    },
    "id": "57ed9000",
    "outputId": "e9ec69cc-5669-48f6-9faf-d0f758927372"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1.\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "669f394398314b6dad5ee346b2df3404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'This is a explanation about how works the pipeline function by Binary Brains', 'labels': ['technology', 'education', 'comedy'], 'scores': [0.8435174822807312, 0.13306769728660583, 0.02341485396027565]}\n",
      "******************+\n",
      "technology: 0.84\n",
      "education: 0.13\n",
      "comedy: 0.02\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "resultado = classifier(\n",
    "    \"This is a explanation about how works the pipeline function by Binary Brains\",\n",
    "    candidate_labels=[\"education\", \"comedy\", \"technology\"],\n",
    ")\n",
    "\n",
    "print(resultado)\n",
    "print(\"******************+\")\n",
    "for label, score in zip(resultado[\"labels\"], resultado[\"scores\"]):\n",
    "    print(f\"{label}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "If-ue12bTpJR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295,
     "referenced_widgets": [
      "c86a46692502459fa66bbe6c4c794cc7",
      "3b2de2dd5bae4a7291a60b488d1a1de4",
      "827b63eb4a0e4191b0c4fcafb9b44826",
      "941f36c2ff814a509a8737f328f7d589",
      "54a308bfe6bc4785b2e33ef02defd6ab",
      "63d6664d2b514147ae289332e79c3912",
      "92877bc6fcca4aa1a73f989d321425fb",
      "1335ec2b3ee44a2fac964f52f085ee27",
      "4eb93a951ec54324b6b77c796d75a6ea",
      "2191859163564663980693b7369617e6",
      "27fb969a8acb4bc9a1e2506b097f3684"
     ]
    },
    "id": "If-ue12bTpJR",
    "outputId": "b1b3b125-a909-49f6-f64a-1028b4b08ee5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d.\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c86a46692502459fa66bbe6c4c794cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel LOAD REPORT from: openai-community/gpt2\n",
      "Key                  | Status     |  | \n",
      "---------------------+------------+--+-\n",
      "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Passing `generation_config` together with generation-related arguments=({'max_new_tokens'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=20) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Mexico is a city of about 900,000 people located between the Alps and Europe.\\n\\nThe country is home'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generador = pipeline(task=\"text-generation\", max_new_tokens = 20)\n",
    "resultado = generador(\"Mexico is a\")\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "rW8q0zMcU2KX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rW8q0zMcU2KX",
    "outputId": "4691bd2e-a138-439f-9fce-f8b3d0c933c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Mexico is a city of about 900,000 people located between the Alps and Europe.\\n\\nThe country is home'}]\n",
      "**********\n",
      "Mexico is a city of about 900,000 people located between the Alps and Europe.\n",
      "\n",
      "The country is home\n"
     ]
    }
   ],
   "source": [
    "print(resultado)\n",
    "print('*'*10)\n",
    "print(resultado[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997a3151",
   "metadata": {
    "id": "997a3151"
   },
   "source": [
    "En los ejemplos anteriore lo que estar√° haciendo el pipeline es elegir un modelo por default, pero tambi√©n se le puede configurar uno en espec√≠fico ü§ó\n",
    "\n",
    "Para saber qu√© modelo usar para cada tarea que quieras realizar puedes buscarlo en el Hub de Hugging Face ‚öíÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f68b1cf5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138,
     "referenced_widgets": [
      "ce333c5c88ae4cc18ddcdf1836a4ada3",
      "9f9497b196954ed89c00ddea4155d1be",
      "dceee10f10824e55ac14fcb06913e2fc",
      "662b7e28222b482c9e64db7b3beadcb2",
      "a56adc70f23049dab467a5fe59f54394",
      "22a745c0842f47ac93533edc32167d1d",
      "0bc1f25c2e8649168e0ce75fc15fe0eb",
      "a6967fd75c6c4dd6ac6d33ce4330c96f",
      "3c1c56e2c8d745fb8750e248878e523c",
      "3230602739e647bc8981b0f5941e954c",
      "e397de0f1e544160a9ec7c20ed8239de"
     ]
    },
    "id": "f68b1cf5",
    "outputId": "7af982eb-2771-4ac4-c5bc-99cde58cf1c1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce333c5c88ae4cc18ddcdf1836a4ada3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/290 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing `generation_config` together with generation-related arguments=({'num_return_sequences', 'max_new_tokens'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Both `max_new_tokens` (=20) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Mexico is a 3rd world country, and the problems are getting worse and worse and worse, and the government just'}, {'generated_text': 'Mexico is a \\nlarge country, and contains a great variety of natural resources, \\nboth mineral and agricultural,'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"HuggingFaceTB/SmolLM2-360M\")\n",
    "resultado = generator(\n",
    "    \"Mexico is a \",\n",
    "    max_new_tokens = 20,\n",
    "    num_return_sequences=2, #n√∫mero de ejemplos que generar√°\n",
    ")\n",
    "print(resultado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hnH0vnhAXKwG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hnH0vnhAXKwG",
    "outputId": "eb1784ea-466d-4e4c-8e28-f9cb5fba4838"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La 1ra oraci√≥n es: Mexico is a 3rd world country, and the problems are getting worse and worse and worse, and the government just\n",
      "La 2do oraci√≥n es: Mexico is a \n",
      "large country, and contains a great variety of natural resources, \n",
      "both mineral and agricultural,\n"
     ]
    }
   ],
   "source": [
    "print('La 1ra oraci√≥n es:',resultado[0]['generated_text'])\n",
    "print('La 2do oraci√≥n es:',resultado[1]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fd0ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformes import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\")\n",
    "unmasker(\"This course will teach you by <mask>\", top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "nYi49dqlZLlj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260,
     "referenced_widgets": [
      "2c10625524f64b9bb6ad8bd414890dde",
      "6c56084d8dc84ba8b509d41d4190c081",
      "765012dc26484b79abd61a5b6a616548",
      "60e6a8982e784295af4b05fdde8334aa",
      "6cec561a00fb42428dfe31939813e539",
      "83cc430c46c04a798025cebd33a05152",
      "16d25e2ee5a84f76a751542e44d93e2f",
      "93a308faec054186bd7437f4109bf0ea",
      "9e8cb948a19e4513850438cc713304a1",
      "f0772964d7114cad91d20dbfba8ab3c0",
      "9ced7dc32fd945ab995e4dd928f54986"
     ]
    },
    "id": "nYi49dqlZLlj",
    "outputId": "5e9c8dab-6a6b-487b-8f55-92a34f7f0f59"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c10625524f64b9bb6ad8bd414890dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/788 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tied weights mapping and config for this model specifies to tie text_decoder.bert.embeddings.word_embeddings.weight to text_decoder.cls.predictions.decoder.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "BlipForQuestionAnswering LOAD REPORT from: Salesforce/blip-vqa-base\n",
      "Key                                       | Status     |  | \n",
      "------------------------------------------+------------+--+-\n",
      "text_encoder.embeddings.position_ids      | UNEXPECTED |  | \n",
      "text_decoder.bert.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Both `max_new_tokens` (=10) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'answer': 'dog'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipeline = pipeline(task=\"visual-question-answering\", model=\"Salesforce/blip-vqa-base\", max_new_tokens = 10)\n",
    "resultado = pipeline(\n",
    "    image=\"/content/ChatGPT Image 21 feb 2026, 04_52_22 p.m..png\",\n",
    "    question=\"Describe the image\",\n",
    ")\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3nFECEu0bv0L",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3nFECEu0bv0L",
    "outputId": "13da1ea6-be63-4abd-c021-e8b841d52bbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "print(resultado[0]['answer'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
