{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92f78b5d",
   "metadata": {
    "id": "92f78b5d"
   },
   "source": [
    "# Funci\u00f3n Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d443e568",
   "metadata": {
    "id": "d443e568"
   },
   "source": [
    "El objeto m\u00e1s b\u00e1sico en la Librer\u00eda Transformers es la funci\u00f3n \"pipeline()\"\ud83e\udd17\n",
    "\n",
    "Este conecta a un modelo a trav\u00e9s de un atajo que te permite usar modelos de Hugging Face ya entrenados sin tener que preocuparte por todo el c\u00f3digo de preprocesamiento y posprocesamiento.\n",
    "\n",
    "En otras palabras, envuelve un modelo junto con su tokenizador y los pasos necesarios para que solo le des un input y recibas una salida lista."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda22ba4",
   "metadata": {
    "id": "cda22ba4"
   },
   "source": [
    "### Instalaci\u00f3n de Transformers"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YbKpfkreN6nL",
    "outputId": "537dee74-a210-4f58-ac91-bc27aba0e380"
   },
   "id": "YbKpfkreN6nL",
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.24.2)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.24.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: typer>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (0.24.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
      "Requirement already satisfied: click>=8.2.1 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers) (8.3.1)\n",
      "Requirement already satisfied: rich>=12.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers) (13.9.4)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers) (0.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (0.1.2)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7df8d9",
   "metadata": {
    "id": "2f7df8d9"
   },
   "source": [
    "### Uso de Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6fa70",
   "metadata": {
    "id": "2ee6fa70"
   },
   "source": [
    "\ud83d\ude80 Tareas soportadas por `pipeline()` en Hugging Face Transformers \ud83e\udd17\n",
    "\n",
    "\ud83d\udcc4 Pipelines de Texto\n",
    "- **text-generation** \u2192 Generar texto a partir de un prompt.  \n",
    "- **text-classification** \u2192 Clasificar texto en categor\u00edas predefinidas.  \n",
    "- **summarization** \u2192 Crear una versi\u00f3n m\u00e1s corta de un texto manteniendo la informaci\u00f3n clave.  \n",
    "- **translation** \u2192 Traducir texto de un idioma a otro.  \n",
    "- **zero-shot-classification** \u2192 Clasificar texto sin entrenamiento previo en etiquetas espec\u00edficas.  \n",
    "- **feature-extraction** \u2192 Extraer representaciones vectoriales de un texto.  \n",
    "\n",
    "---\n",
    "\n",
    "\ud83d\uddbc\ufe0f Pipelines de Imagen\n",
    "- **image-to-text** \u2192 Generar descripciones de texto a partir de im\u00e1genes.  \n",
    "- **image-classification** \u2192 Identificar objetos en una imagen.  \n",
    "- **object-detection** \u2192 Localizar e identificar objetos en im\u00e1genes.  \n",
    "\n",
    "---\n",
    "\n",
    "\ud83d\udd0a Pipelines de Audio\n",
    "- **automatic-speech-recognition** \u2192 Convertir voz en texto.  \n",
    "- **audio-classification** \u2192 Clasificar audio en categor\u00edas.  \n",
    "- **text-to-speech** \u2192 Convertir texto en audio hablado.  \n",
    "\n",
    "---\n",
    "\n",
    "\ud83c\udfaf Pipelines Multimodales\n",
    "- **image-text-to-text** \u2192 Responder a una imagen bas\u00e1ndose en un prompt de texto.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa2f750f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "a2b64606b9c745479f3fb220ab0b8948",
      "3e739ef571c1472ca164f1c4a27005ab",
      "f6c53469945948c0802be20d6ee62487",
      "7799e0bca28e4fc8885c4ae87e38370d",
      "86650b5c1d1645429a9daad9687fe1d5",
      "0167b36799254a7ea007ab71b4ebddac",
      "b15c51f20ada4c9eb75a511093ce664a",
      "40a093d9f0dc452691c4746f63192900",
      "0298783b5d184e1b99d2ba2e2f43af67",
      "b1d205150fab451daf97452ae80392a3",
      "f2f7b5bb4e914f6e8eb36b55cca8f62e"
     ]
    },
    "id": "aa2f750f",
    "outputId": "25f34859-de4a-480c-b097-d1540263f828"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f.\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading weights:   0%|          | 0/104 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a2b64606b9c745479f3fb220ab0b8948"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9989345669746399}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"I love using transformers library!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57ed9000",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191,
     "referenced_widgets": [
      "669f394398314b6dad5ee346b2df3404",
      "17d8322046824dae93f4d2391d121d4c",
      "8a9a3644706441a0a9aca5ef7b95ff17",
      "107a07ac2693472ebe843de164509076",
      "98e4184b049143e0b6af94f843e8fd62",
      "94ed3934e3404e9c9e74898de437d7c0",
      "7e1a6484a3a044b1b9cfdc612c77cb4d",
      "008f6a2b95d54ff88ba7e74191e15a8d",
      "8015514d014743738a7b2646b0e4d18e",
      "6b8723b8ca23460e89812d9cf30fe8da",
      "e0b0e0c4b04045e683de3f70c02c91bc"
     ]
    },
    "id": "57ed9000",
    "outputId": "e9ec69cc-5669-48f6-9faf-d0f758927372"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1.\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading weights:   0%|          | 0/515 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "669f394398314b6dad5ee346b2df3404"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'sequence': 'This is a explanation about how works the pipeline function by Binary Brains', 'labels': ['technology', 'education', 'comedy'], 'scores': [0.8435174822807312, 0.13306769728660583, 0.02341485396027565]}\n",
      "******************+\n",
      "technology: 0.84\n",
      "education: 0.13\n",
      "comedy: 0.02\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "resultado = classifier(\n",
    "    \"This is a explanation about how works the pipeline function by Binary Brains\",\n",
    "    candidate_labels=[\"education\", \"comedy\", \"technology\"],\n",
    ")\n",
    "\n",
    "print(resultado)\n",
    "print(\"******************+\")\n",
    "for label, score in zip(resultado[\"labels\"], resultado[\"scores\"]):\n",
    "    print(f\"{label}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generador = pipeline(task=\"text-generation\", max_new_tokens = 20)\n",
    "resultado = generador(\"Mexico is a\")\n",
    "print(resultado)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295,
     "referenced_widgets": [
      "c86a46692502459fa66bbe6c4c794cc7",
      "3b2de2dd5bae4a7291a60b488d1a1de4",
      "827b63eb4a0e4191b0c4fcafb9b44826",
      "941f36c2ff814a509a8737f328f7d589",
      "54a308bfe6bc4785b2e33ef02defd6ab",
      "63d6664d2b514147ae289332e79c3912",
      "92877bc6fcca4aa1a73f989d321425fb",
      "1335ec2b3ee44a2fac964f52f085ee27",
      "4eb93a951ec54324b6b77c796d75a6ea",
      "2191859163564663980693b7369617e6",
      "27fb969a8acb4bc9a1e2506b097f3684"
     ]
    },
    "id": "If-ue12bTpJR",
    "outputId": "b1b3b125-a909-49f6-f64a-1028b4b08ee5"
   },
   "id": "If-ue12bTpJR",
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d.\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c86a46692502459fa66bbe6c4c794cc7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPT2LMHeadModel LOAD REPORT from: openai-community/gpt2\n",
      "Key                  | Status     |  | \n",
      "---------------------+------------+--+-\n",
      "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Passing `generation_config` together with generation-related arguments=({'max_new_tokens'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=20) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'generated_text': 'Mexico is a city of about 900,000 people located between the Alps and Europe.\\n\\nThe country is home'}]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(resultado)\n",
    "print('*'*10)\n",
    "print(resultado[0][\"generated_text\"])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rW8q0zMcU2KX",
    "outputId": "4691bd2e-a138-439f-9fce-f8b3d0c933c0"
   },
   "id": "rW8q0zMcU2KX",
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'generated_text': 'Mexico is a city of about 900,000 people located between the Alps and Europe.\\n\\nThe country is home'}]\n",
      "**********\n",
      "Mexico is a city of about 900,000 people located between the Alps and Europe.\n",
      "\n",
      "The country is home\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997a3151",
   "metadata": {
    "id": "997a3151"
   },
   "source": [
    "En los ejemplos anteriore lo que estar\u00e1 haciendo el pipeline es elegir un modelo por default, pero tambi\u00e9n se le puede configurar uno en espec\u00edfico \ud83e\udd17\n",
    "\n",
    "Para saber qu\u00e9 modelo usar para cada tarea que quieras realizar puedes buscarlo en el Hub de Hugging Face \u2692\ufe0f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f68b1cf5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138,
     "referenced_widgets": [
      "ce333c5c88ae4cc18ddcdf1836a4ada3",
      "9f9497b196954ed89c00ddea4155d1be",
      "dceee10f10824e55ac14fcb06913e2fc",
      "662b7e28222b482c9e64db7b3beadcb2",
      "a56adc70f23049dab467a5fe59f54394",
      "22a745c0842f47ac93533edc32167d1d",
      "0bc1f25c2e8649168e0ce75fc15fe0eb",
      "a6967fd75c6c4dd6ac6d33ce4330c96f",
      "3c1c56e2c8d745fb8750e248878e523c",
      "3230602739e647bc8981b0f5941e954c",
      "e397de0f1e544160a9ec7c20ed8239de"
     ]
    },
    "id": "f68b1cf5",
    "outputId": "7af982eb-2771-4ac4-c5bc-99cde58cf1c1"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading weights:   0%|          | 0/290 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce333c5c88ae4cc18ddcdf1836a4ada3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Passing `generation_config` together with generation-related arguments=({'num_return_sequences', 'max_new_tokens'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Both `max_new_tokens` (=20) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'generated_text': 'Mexico is a 3rd world country, and the problems are getting worse and worse and worse, and the government just'}, {'generated_text': 'Mexico is a \\nlarge country, and contains a great variety of natural resources, \\nboth mineral and agricultural,'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"HuggingFaceTB/SmolLM2-360M\")\n",
    "resultado = generator(\n",
    "    \"Mexico is a \",\n",
    "    max_new_tokens = 20,\n",
    "    num_return_sequences=2, #n\u00famero de ejemplos que generar\u00e1\n",
    ")\n",
    "print(resultado)\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print('La 1ra oraci\u00f3n es:',resultado[0]['generated_text'])\n",
    "print('La 2do oraci\u00f3n es:',resultado[1]['generated_text'])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hnH0vnhAXKwG",
    "outputId": "eb1784ea-466d-4e4c-8e28-f9cb5fba4838"
   },
   "id": "hnH0vnhAXKwG",
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "La 1ra oraci\u00f3n es: Mexico is a 3rd world country, and the problems are getting worse and worse and worse, and the government just\n",
      "La 2do oraci\u00f3n es: Mexico is a \n",
      "large country, and contains a great variety of natural resources, \n",
      "both mineral and agricultural,\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipeline = pipeline(task=\"visual-question-answering\", model=\"Salesforce/blip-vqa-base\", max_new_tokens = 10)\n",
    "resultado = pipeline(\n",
    "    image=\"/content/ChatGPT Image 21 feb 2026, 04_52_22 p.m..png\",\n",
    "    question=\"Describe the image\",\n",
    ")\n",
    "print(resultado)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260,
     "referenced_widgets": [
      "2c10625524f64b9bb6ad8bd414890dde",
      "6c56084d8dc84ba8b509d41d4190c081",
      "765012dc26484b79abd61a5b6a616548",
      "60e6a8982e784295af4b05fdde8334aa",
      "6cec561a00fb42428dfe31939813e539",
      "83cc430c46c04a798025cebd33a05152",
      "16d25e2ee5a84f76a751542e44d93e2f",
      "93a308faec054186bd7437f4109bf0ea",
      "9e8cb948a19e4513850438cc713304a1",
      "f0772964d7114cad91d20dbfba8ab3c0",
      "9ced7dc32fd945ab995e4dd928f54986"
     ]
    },
    "id": "nYi49dqlZLlj",
    "outputId": "5e9c8dab-6a6b-487b-8f55-92a34f7f0f59"
   },
   "id": "nYi49dqlZLlj",
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading weights:   0%|          | 0/788 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2c10625524f64b9bb6ad8bd414890dde"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The tied weights mapping and config for this model specifies to tie text_decoder.bert.embeddings.word_embeddings.weight to text_decoder.cls.predictions.decoder.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "BlipForQuestionAnswering LOAD REPORT from: Salesforce/blip-vqa-base\n",
      "Key                                       | Status     |  | \n",
      "------------------------------------------+------------+--+-\n",
      "text_encoder.embeddings.position_ids      | UNEXPECTED |  | \n",
      "text_decoder.bert.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Both `max_new_tokens` (=10) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'answer': 'dog'}]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(resultado[0]['answer'])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3nFECEu0bv0L",
    "outputId": "13da1ea6-be63-4abd-c021-e8b841d52bbe"
   },
   "id": "3nFECEu0bv0L",
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dog\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
